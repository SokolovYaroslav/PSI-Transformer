save_path: pretrained/

source_data:
#  train_jsonl: /Users/Yaroslav.Sokolov/work/psi_data/java_small_out/java_small_out.train.jsonl
  train_jsonl: /Users/Yaroslav.Sokolov/work/psi_data/java_small_out/java_small_out.val.jsonl
#  train_jsonl: /Users/Yaroslav.Sokolov/work/psiminer/mock_data/mock_data.train.jsonl
  val_jsonl: /Users/Yaroslav.Sokolov/work/psi_data/java_small_out/java_small_out.val.jsonl
  test_jsonl: /Users/Yaroslav.Sokolov/work/psi_data/java_small_out/java_small_out.test.jsonl
  mock_jsonl: /Users/Yaroslav.Sokolov/work/psiminer/mock_data/mock_data.train.jsonl

psi_pretraining:
  overwrite: True

  max_percentile: 99
  transformations:
    - children_amount_normalization

  # Turns on debug output after training PSI datapoint
  show_mock: True

tokenizer:
  vocab_size: 16384
  min_frequency: 100
  dropout: 0

dataset:
  # Number of files which will be loaded and shuffled
  # Larger number => better shuffling & bigger memory consumption and dataset latency
  shuffle_bucket: 500
  # Examples will be sliced into pieces due to model's capacities
  # Researchers often do slicing with overlapping so model can see some history for every token
  overlap_slicing: 0.1
  # Whether to pad labels for overlapped tokens
  pad_overlapped: True

model:
  type: "gpt-2"
  hidden_size: 128
  n_layers: 1
  context_length: 64
  labels_pad: -100  # HF format

training:
  # Learning rate for single example (will be scaled according to total batch size)
  base_lr: 0.0000078125
  adam_eps: 1e-8

  epochs: 5
  batch_size: 6
  grad_accumulation_steps: 1
  max_grad_norm: 1.0
  warmup_tokens: 50000000
  weight_decay: 0.1

  val_check_interval: 0.05
  save_top_k: 10

  fp16: True
  fp16_opt_level: "O1"
  num_dataset_workers: 1
  local_rank: ???
  world_size: ???
  # 0 means CPU only
  n_gpus: 0
  seed: 0xB1B7E

hydra:
  run:
    dir: .
  output_subdir: null
  job_logging: null
  hydra_logging: null

