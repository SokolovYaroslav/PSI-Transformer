save_path: out/

source_data:
  train_jsonl: data/java-med-out/java-med-out.train.jsonl
#  train_jsonl: data/java-small-out/java-small-out.val.jsonl
  val_jsonl: data/java-med-out/java-med-out.val.jsonl
#  val_jsonl: data/java-small-out/java-small-out.val.jsonl
  test_jsonl: data/java-med-out/java-med-out.test.jsonl
#  test_jsonl: data/java-small-out/java-small-out.test.jsonl
  mock_jsonl: data/mock-out/mock-out.train.jsonl

psi_pretraining:
  overwrite: False

  max_percentile: 99
  transformations:
    - children_amount_normalization

  # Turns on debug output after training PSI datapoint
  show_mock: True

tokenizer:
  vocab_size: 16384
  min_frequency: 100
  dropout: 0

dataset:
  # Number of files which will be loaded and shuffled
  # Larger number => better shuffling & bigger memory consumption and dataset latency
  shuffle_bucket: 5000
  # Examples will be sliced into pieces due to model's capacities
  # Researchers often do slicing with overlapping so model can see some history for every token
  overlap_slicing: 0.1
  # Whether to pad labels for overlapped tokens
  pad_overlapped: True

model:
  type: "gpt-2"
  hidden_size: 1024
  n_layers: 18
  context_length: 384
  labels_pad: -100  # HF format

training:
  # Path to .ckpt file or Nothing (None isn't working)
  # Path can be in "%datetime%/%name%.ckpt" format or just regular path
  resume_from_checkpoint:
  # Learning rate for single example (will be scaled according to total batch size)
  base_lr: 0.0000078125
  adam_eps: 1e-8

  epochs: 5
  batch_size: 6
  grad_accumulation_steps: 1
  max_grad_norm: 1.0
  warmup_tokens: 50000000
  weight_decay: 0.1

  val_check_interval: 0.1
  save_top_k: 30

  fp16: True
  fp16_opt_level: "O1"
  num_dataset_workers: 7
  local_rank: ???  # means cannot be read before assignment
  world_size: ???  # means cannot be read before assignment
  n_gpus: 8  # 0 means CPU only
  seed: 0xB1B7E
